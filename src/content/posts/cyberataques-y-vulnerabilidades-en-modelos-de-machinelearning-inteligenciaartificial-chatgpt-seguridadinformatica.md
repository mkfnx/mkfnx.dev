---
title: "Cyberataques Y Vulnerabilidades En Modelos De  Machinelearning  Inteligenciaartificial  Chatgpt  Seguridadinformatica"
description: "cyberataques y vulnerabilidades en modelos de  machinelearning  inteligenciaartificial  chatgpt  seguridadinformatica"
date: 2024-08-04
image: "/images/posts/01.jpg"
categories: ['']
authors: ['Miguel López']
tags: ['CyberAtaques', 'MachineLearning', 'InteligenciaArtificial', 'ChatGPT', 'SeguridadInformática']
draft: True
slug: "cyberataques-y-vulnerabilidades-en-modelos-de-machinelearning-inteligenciaartificial-chatgpt-seguridadinformatica"
---

<blockquote class="tiktok-embed" cite="{https://www.tiktok.com/@mkfnx/video/7325601096636845317}" data-video-id="7325601096636845317" style="max-width: 605px;min-width: 325px;" > <section> <a target="_blank" title="@mkfnx" href="https://www.tiktok.com/@mkfnx?refer=embed">@mkfnx</a> cyberataques y  </section> <a title="CyberAtaques" target="_blank" href="https://www.tiktok.com/tag/CyberAtaques?refer=embed">#CyberAtaques</a><a title="MachineLearning" target="_blank" href="https://www.tiktok.com/tag/MachineLearning?refer=embed">#MachineLearning</a><a title="InteligenciaArtificial" target="_blank" href="https://www.tiktok.com/tag/InteligenciaArtificial?refer=embed">#InteligenciaArtificial</a><a title="ChatGPT" target="_blank" href="https://www.tiktok.com/tag/ChatGPT?refer=embed">#ChatGPT</a><a title="SeguridadInformática" target="_blank" href="https://www.tiktok.com/tag/SeguridadInformática?refer=embed">#SeguridadInformática</a> </blockquote> <script async src="https://www.tiktok.com/embed.js"></script>

ataques y vulnerabilidades en modelos de machine learning como muchos tipos de software los modelos de machine learning no están exentos de tener vulnerabilidades decir pueden ser manipulados por alguien para afectar su funcionamiento de forma que se aleje del objetivo original y en este post del blog de R. Nikhil se hace una revisión de los más comunes el primero es un ataque con entradas adversariales que consiste en fabricar una entrada que cause que el modelo falle en su función en en este ejemplo vemos cómo se puede agregar ruido a una imagen para afectar su clasificación también podría hacerse agregando un parche como en este ejemplo y así poder implementarse incluso de forma física a través de un sticker por ejemplo hasta en señales de tránsito lo cual podría afectar a vehículos autónomos la clasificación de imágenes es el ejemplo más conocido de esta vulnerabilidad pero ya existe investigación y pruebas  para otro tipo de aplicaciones como antivirus conversión de voz a texto o procesamiento de lenguaje ataque por envenenamiento de datos y de puerta trasera consiste en atacar los datos de entrenamiento y se volvió popular cuando se reveló que modelos como chatGPT hicieron scraping de sitios de internet hubo gente que a su sitio agregó  instrucciones dirigidas al modelo para afectar su comportamiento e incluso llegó a considerarse una defensa de los artistas para evitar que su trabajo fuera incorporado en modelos sin su autorización este ataque se comporta en ocasiones como backdoor debido a que hace uso de los datos infiltrados en el entrenamiento para afectar el funcionamiento del modelo ataque de extracción de modelo en este ataque se intenta de alguna forma robar el modelo o al menos su funcionamiento ya que consiste en entrenar un modelo propio usando las respuestas de otro esto fue aprovechado por modelos open Source como Alpaca, que usó el API de GPT para ajustar sus resultados esta técnica permite ahorrar en costos de entrenamiento de modelos y ayudar a combatir la ventaja de empresas con gran acceso a financiamiento ataque de inferencia de pertenencia este ataque se usa para descubrir los datos de entrenamiento del modelo lo cual puede ayudar a facilitar alguno de los otros ataques o incluso revelar información confidencial para determinar si un dato se usa en el modelo se entrenan varios modelos alternativos que alternan entre contener o no el dato que se investiga luego se evalúa el desempeño de cada modelo para evaluar cómo la presencia del dato en investigación afecta el desempeño del modelo a lo largo de todo el post se incluyen links a artículos de investigación y en el artículo de este ataque se realizan pruebas contra modelos de Amazon y Google otro es el fair washing que está derivado del término White washing este ataque se enfoca en producir explicaciones sobre los resultados proporcionados por el modelo con el fin de dar la apariencia de que son justos o equilibrados, aunque no lo sean también hay ataques más tradicionales  como los de denegación de servicio pero usando una variante de la entrada adversarial ya que en este caso la entrada maliciosa tiene el objetivo de incrementar de gran manera el tiempo de procesamiento y los costos de energía la solución de estas vulnerabilidades ha resultado muy compleja ya que no es una vulnerabilidad tradicional de código sino que más bien está en los datos de entrenamiento o entrada del modelo y hasta cierto punto parecen ser  inherentes del proceso de aprendizaje y búsqueda de patrones de los modelos ya hablé un poco sobre esto en un video anterior que dejo enlazado en la descripción comenta si has visto ejemplos de este tipo de ataques y cómo crees que podrían solucionarse 