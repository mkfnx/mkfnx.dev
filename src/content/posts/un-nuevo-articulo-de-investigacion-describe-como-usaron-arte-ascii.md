---
title: "Un Nuevo Articulo De Investigacion Describe Como Usaron Arte Ascii"
description: "un nuevo articulo de investigacion describe como usaron arte ascii"
date: 2024-08-04
image: "/images/posts/04.jpg"
categories: ['']
authors: ['Miguel LÃ³pez']
tags: ['chatGPT', 'Gemini', 'InteligenciaArtificial', 'AI', 'MachineLearning']
draft: True
slug: "un-nuevo-articulo-de-investigacion-describe-como-usaron-arte-ascii"
---

<blockquote class="tiktok-embed" cite="{https://www.tiktok.com/@mkfnx/video/7345231109212376326}" data-video-id="7345231109212376326" style="max-width: 605px;min-width: 325px;" > <section> <a target="_blank" title="@mkfnx" href="https://www.tiktok.com/@mkfnx?refer=embed">@mkfnx</a> un nuevo articu </section> <a title="chatGPT" target="_blank" href="https://www.tiktok.com/tag/chatGPT?refer=embed">#chatGPT</a><a title="Gemini" target="_blank" href="https://www.tiktok.com/tag/Gemini?refer=embed">#Gemini</a><a title="InteligenciaArtificial" target="_blank" href="https://www.tiktok.com/tag/InteligenciaArtificial?refer=embed">#InteligenciaArtificial</a><a title="AI" target="_blank" href="https://www.tiktok.com/tag/AI?refer=embed">#AI</a><a title="MachineLearning" target="_blank" href="https://www.tiktok.com/tag/MachineLearning?refer=embed">#MachineLearning</a> </blockquote> <script async src="https://www.tiktok.com/embed.js"></script>

Cheating ChatGPT with ASCII art.  This article called ArtPrompt describes how they used  ASCII art to Jailbreak ChatGPT and other great language models.  This technique consists of replacing words or terms that are  limited by templates and changing them for the ASCII art equivalent.  In this way the templates respond to requests  that may be insecure and would normally be blocked.  This technique is effective on better known models such as GPT,  Claude Gemini and,  and is on average more effective than previously known techniques.  The technique consists of two steps,  one to mask the word and the other to generate a prompt using the masked word.  We can see in this example.  This includes the ASCII art with the masked  word and instructs the model to decipher the word.  The second part is to ask the model to use the  word decrypted in certain parts marked in the prompt,  along with some instructions to prevent it from  repeating it and not to activate the security filters.  The article includes many references about  the work of previous Jailbreak techniques,  about the defenses for this type of attacks and even about the datasets used,  and you can consult it in the link shown. 